scale_y_log10()
pq_snt <- pq_dat %>%
unnest_sentences(output = sentences, input = answers)
pq_snt_s <- pq_snt[c(1:6),]
knitr::kable(pq_snt_s,format="html") %>%
html_table_width(c(100,100,400))
###############################################################################
#Text Analysis of a Research Article
###############################################################################
# Setting up libraries
require(librarian)
librarian::shelf(dplyr, tidytext, tidyverse,
widyr,igraph, ggraph,
wordcloud, reshape2, graphlayouts,
pluralize, quanteda, qgraph, cowplot, readr, pdftools)
ppr <- pdf_text("assets/wq_burning1.pdf")
ppr <- pdf_text("assets/wq_burning1.pdf")
ppr_cln <- ppr %>%
str_split("\n") %>%
unlist() %>%
str_to_lower() %>%
str_replace_all("\n"," ") %>%
str_remove_all(",") %>%
str_replace_all("\\s{2,}", " ") %>%
str_replace_all("[:digit:]", " ") %>%
str_replace_all("[:punct:]", " ") %>%
str_trim()
head(ppr_cln)
str(ppr)
head(ppr)
toc <- pdf_toc("assets/wq_burning1.pdf")
View(toc)
jsonlite::toJSON(toc, auto_unbox = TRUE, pretty = TRUE)
info <- pdf_info("assets/wq_burning1.pdf")
info
kwrd <- ppr$keys$keywords
kwrd <- ppr$keys
ppr$keys
View(info)
info$keys
info$keys$Subject
info$keys$Subject
subject <- info$keys$Subject
subject
ppr_cln <- ppr %>%
str_detect(ppr,subject)
ppr_cln <- ppr %>%
str_detect(ppr,subject,negate = FALSE)
subject
paste(subject)
ppr_cln <- ppr %>%
str_detect(ppr,paste(subject),negate = FALSE)
ppr_cln <- ppr %>%
str_detect(ppr,paste(subject, collapse = "|"),negate = FALSE)
subject <- c(info$keys$Subject)
subject
ppr_cln <- ppr %>%
str_detect(ppr,paste(subject),negate = FALSE)
librarian::shelf(dplyr, tidytext, tidyverse,stringr,
widyr,igraph, ggraph,
wordcloud, reshape2, graphlayouts,
pluralize, quanteda, qgraph, cowplot, readr, pdftools)
subject <- c(info$keys$Subject)
ppr_cln <- ppr %>%
str_detect(ppr,paste(subject),negate = FALSE)
subject <- c(info$keys$Subject)
ppr_cln <- ppr %>%
str_detect(ppr,paste(subject),negate = FALSE)
paste(subject)
ppr_cln <- ppr %>%
str_detect(ppr,"Nature Communications, doi:10.1038/s41467-021-22747-3")
ppr_cln <- ppr %>%
str_detect(ppr,"Nature")
unestokens(subject)
unnest_tokens(output = words, input= subject)
unnest_tokens(output = words, input= tbl(subject))
subject
unnest_tokens_(output = words, input= subject)
unnest_tokens(subject,txt)
str(subject)
subject <- as.string(info$keys$Subject)
subject <- info$keys$Subject
str(subject)
paste(subject, collapse = " ")
a <- paste(subject, collapse = " ")
str(a)
a <- paste(subject, collapse = ' ')
a
stra(a)
str(a)
install.packages("stringi")
librarian::shelf(dplyr, tidytext, tidyverse,stringr,stringi,
widyr,igraph, ggraph,
wordcloud, reshape2, graphlayouts,
pluralize, quanteda, qgraph, cowplot, readr, pdftools)
a <- toString(subject)
a
str(a)
# Reading our file
ppr <- pdf_text("assets/wq_burning1.pdf")
info <- pdf_info("assets/wq_burning1.pdf")
ppr_txt <- data.frame(line = 1:8, text = ppr)
subject <- info$keys$Subject
ppr_cln <- ppr_txt %>%
str_to_lower() %>%
unnest_tokens(output = word, input = text)
ppr_cln <- ppr_txt %>%
unnest_tokens(output = word, input = text)
ppr_cln <- ppr_txt %>%
unnest_tokens(output = word, input = text) %>%
filter(str_detect("Nature"))
ppr_cln <- ppr_txt %>%
unnest_tokens(output = word, input = text) %>%
filter(str_detect(word,"Nature"))
ppr_cln <- ppr_txt %>%
unnest_tokens(output = word, input = text) %>%
filter(str_detect(word,paste(subject)))
subject
domain <- info$keys$`CrossMarkDomains[1]`
domain
pages <- info$pages
pages
subject <- info$keys$Subject
domain <- info$keys$`CrossMarkDomains[1]`
creator <- info$keys$Creator
doi <- info$keys$doi
cdomain <- info$keys$`CrossMarkDomains[2]`
ppr_cln <- ppr_txt %>%
unnest_tokens(output = word, input = text) %>%
filter(str_detect(word,paste(subject))) %>%
filter(str_detect(word,paste(domain))) %>%
filter(str_detect(word,paste(creator))) %>%
filter(str_detect(word,paste(doi))) %>%
filter(str_detect(word,paste(cdomain)))
ppr_cln <- ppr_txt %>%
unnest_tokens(output = word, input = text) %>%
filter(str_detect(word,paste(subject))) %>%
filter(str_detect(word,paste(domain))) %>%
filter(str_detect(word,paste(creator))) %>%
filter(str_detect(word,paste(doi))) %>%
filter(str_detect(word,paste(cdomain))) %>%
anti_join(stop_words, by = "word") %>%
filter(str_detect(word,"[:alpha:]")) %>%
count(word, sort = TRUE) %>%
mutate(length = nchar(word)) %>%
select(word, n) %>%
mutate(rank = row_number(),
total=sum(n),
t_freq = n/total)
ppr_cln %>% with(wordcloud(word,n,max.words = 400, scale = c(3.5,0.35)))
ppr_cln %>% with(wordcloud(word,n))
ppr_cln %>% with(wordcloud(word,n,max.words = 400))
View(ppr_cln)
ppr_cln <- ppr_txt %>%
unnest_tokens(output = word, input = text
ppr_cln <- ppr_txt %>%
ppr_cln <- ppr_txt %>%
unnest_tokens(output = word, input = text)
ppr_cln <- ppr_txt %>%
unnest_tokens(output = word, input = text) %>%
filter(str_detect(word,paste(subject)))
str_detect(word,paste(subject)
str_detect(word,paste(subject))
ppr_cln <- ppr_txt %>%
unnest_tokens(output = word, input = text) %>%
str_detect(word,paste(subject))
paste(domain)
str_detect(ppr_cln$word,paste(subject))
str_detect(ppr_cln$word,paste(domain))
str_replace(ppr_cln$word,paste(domain))
str_replace(ppr_cln$word,paste(domain)," ")
str_replace(ppr_cln$word,paste(domain),"hey")
str_detect()
subject
str_split(paste(suject),collapse="")
str_split(paste(suject))
str_split(paste(suject), collapse = "")
###############################################################################
#Text Analysis of a Research Article
###############################################################################
# Setting up libraries
require(librarian)
librarian::shelf(dplyr, tidytext, tidyverse,stringr,stringi,
widyr,igraph, ggraph,
wordcloud, reshape2, graphlayouts,
pluralize, quanteda, qgraph, cowplot, readr, pdftools)
# Reading our file
ppr <- pdf_text("assets/wq_burning1.pdf")
info <- pdf_info("assets/wq_burning1.pdf")
ppr_txt <- data.frame(line = 1:8, text = ppr)
subject <- info$keys$Subject
a <- strsplit(subject," ")
a
unlist(a)
a <- unlist(strsplit(info$keys$Subject," "))
ppr_cln <- ppr_txt %>%
unnest_tokens(output = word, input = text) %>%
str_detect(word,paste(a))
ppr_cln <- ppr_txt %>%
unnest_tokens(output = word, input = text)
paste(a)
a <- strsplit(info$keys$Subject," ")
paste(a)
a <- c(strsplit(info$keys$Subject," "))
a
paste(a)
ppr_cln <- ppr_txt %>%
unnest_tokens(output = word, input = text) %>%
str_detect(word,paste(a))
a <- unlist(c(strsplit(info$keys$Subject," ")))
a
paste(a)
ppr_cln <- ppr_txt %>%
unnest_tokens(output = word, input = text) %>%
str_detect(word,paste(a))
a <- unlist(strsplit(info$keys$Subject," "))
a
b <- str_replace(a," ",",")
paste(b)
ppr_cln <- ppr_txt %>%
unnest_tokens(output = word, input = text) %>%
str_detect(word,paste(b))
a <- unlist(strsplit(info$keys$Subject,"\\."))
a
ppr_cln <- ppr_txt %>%
unnest_tokens(output = word, input = text) %>%
str_detect(word,paste(a))
a <- c(unlist(strsplit(info$keys$Subject,"\\.")))
a
ppr_cln <- ppr_txt %>%
unnest_tokens(output = word, input = text) %>%
str_detect(word,paste(a))
a <- c(unlist(strsplit(info$keys$Subject)))
a <- unlist(strsplit(info$keys$Subject," "))
a
paste(a,collapse = "|")
paste(a,collapse = ",")
ppr_cln <- ppr_txt %>%
unnest_tokens(output = word, input = text) %>%
filter(str_detect(word,paste(a,collapse = "|")),negate = TRUE)
a
a <- as.vector(unlist(strsplit(info$keys$Subject," ")))
a
paste(a)
head(ppr_txt)
ppr_cln <- ppr_txt %>%
unnest_tokens(output = word, input = text)
head(ppr_cln)
filter(str_detect(ppr_txt,paste(a,collapse = "|")),negate = TRUE)
a
paste(a,sep = " ")
# Cleanning up the file
ppr_txt <- ppr_txt %>%
str_replace(paste(a,sep = " "),"")
ppr <- pdf_text("assets/wq_burning1.pdf")
info <- pdf_info("assets/wq_burning1.pdf")
ppr_txt <- data.frame(line = 1:8, text = ppr)
str(ppr_txt)
# Cleanning up the file
ppr_txt1 <- ppr_txt %>%
str_replace(text,paste(a,sep = " "),"")
# Cleanning up the file
ppr_txt1 <- ppr_txt %>%
str_replace(text,paste(a,sep = " "))
# Cleanning up the file
ppr_txt1 <- ppr_txt %>%
str_replace(text,paste(a),"")
# Cleanning up the file
ppr_txt1 <- ppr_txt %>%
str_replace(text,paste(a))
?a
# Cleanning up the file
ppr_txt1 <- ppr_txt %>%
filter(str_detect(text,"Nature"))
ppr_txt1
# Cleanning up the file
ppr_txt1 <- ppr_txt %>%
filter(str_replace(text,"Nature",""))
# Cleanning up the file
ppr_txt1 <- ppr_txt %>%
str_replace(text,"Nature","")
# Cleanning up the file
ppr_txt1 <- ppr_txt %>%
str_replace(text,"Nature"," ")
# Cleanning up the file
ppr_txt1 <- ppr_txt %>%
str_replace(text,"Nature")
# Cleanning up the file
ppr_txt1 <- ppr_txt %>%
str_remove(text,"Nature")
# Cleanning up the file
ppr_txt1 <- ppr_txt %>%
filter(str_remove(text,"Nature"))
b <- strsplit(info$keys$Subject," ")
b
paste(b,collapse="|")
# Cleanning up the file
ppr_txt1 <- ppr_txt %>%
filter(str_remove_all(text,paste(b,collapse = "|"))))
# Cleanning up the file
ppr_txt1 <- ppr_txt %>%
filter(str_remove_all(text,paste(b,collapse = "|")))
ppr_txt$text
# Cleanning up the file
ppr_txt1 <- ppr_txt %>%
filter(str_remove_all(text,paste(b,collapse = "|")))
ppr_txt2 <- ppr_txt %>% str_remove_all(text,paste(b,collapse = "|"))
str_remove_all(ppr_txt$text,paste(b,collapse = "|"))
ppr_txt2 <- ppr_txt %>%
filter(str_detect(text,paste(b,collapse = "|")))
###############################################################################
#Text Analysis of a Research Article
###############################################################################
# Setting up libraries
require(librarian)
librarian::shelf(dplyr, tidytext, tidyverse,stringr,stringi,
widyr,igraph, ggraph,
wordcloud, reshape2, graphlayouts,
pluralize, quanteda, qgraph, cowplot, readr, pdftools)
# Downloading the file
download.file("https://www.nature.com/articles/s41467-021-22747-3.pdf",
destfile = "~/GitHub/burn_iss-website/assets/wq_burning1.pdf", mode = "wb")
# Reading our file
ppr <- pdf_text("assets/wq_burning1.pdf")
#Text Analysis of a Research Article
###############################################################################
# Setting up libraries
require(librarian)
librarian::shelf(dplyr, tidytext, tidyverse,stringr,stringi,
widyr,igraph, ggraph,
wordcloud, reshape2, graphlayouts,
pluralize, quanteda, qgraph, cowplot, readr, pdftools)
# Reading our file
ppr <- pdf_text("assets/wq_burning1.pdf")
###############################################################################
#Text Analysis of a Research Article
###############################################################################
# Setting up libraries
require(librarian)
librarian::shelf(dplyr, tidytext, tidyverse,stringr,stringi,
widyr,igraph, ggraph,
wordcloud, reshape2, graphlayouts,
pluralize, quanteda, qgraph, cowplot, readr, pdftools)
# # Downloading the file
# download.file("https://www.nature.com/articles/s41467-021-22747-3.pdf",
#               destfile = "~/GitHub/burn_iss-website/assets/wq_burning1.pdf", mode = "wb")
# Reading our file
ppr <- pdf_text("assets/wq_burning.pdf")
info <- pdf_info("assets/wq_burning.pdf")
ppr_txt <- data.frame(line = 1:8, text = ppr)
# Extracting pdf metadata and removing it from text body
sbj <- unlist(strsplit(info$keys$Subject, " "))
dmn <- unlist(strsplit(info$keys$`CrossMarkDomains[1]`," "))
ctr <- unlist(strsplit(info$keys$Creator, " "))
doi <- unlist(strsplit(info$keys$doi, " "))
cdm <- unlist(strsplit(info$keys$`CrossMarkDomains[2]`," "))
# Cleaning pdf file
ppr_txt$text <- sub(paste(sbj, collapse = '|'),'',ppr_txt)
ppr_txt$text <- sub(paste(dmn, collapse = '|'),'',ppr_txt)
ppr_txt$text <- sub(paste(ctr, collapse = '|'),'',ppr_txt)
ppr_cln <- as.data.frame(ppr_txt %>%
str_to_lower() %>%
str_replace_all("\n"," ")
ppr_cln <- as.data.frame(ppr_txt %>%
View(ppr_txt)
###############################################################################
#Text Analysis of a Research Article
###############################################################################
# Setting up libraries
require(librarian)
librarian::shelf(dplyr, tidytext, tidyverse,stringr,stringi,
widyr,igraph, ggraph,
wordcloud, reshape2, graphlayouts,
pluralize, quanteda, qgraph, cowplot, readr, pdftools)
# # Downloading the file
# download.file("https://www.nature.com/articles/s41467-021-22747-3.pdf",
#               destfile = "~/GitHub/burn_iss-website/assets/wq_burning1.pdf", mode = "wb")
# Reading our file
ppr <- pdf_text("assets/wq_burning1.pdf")
# Reading our file
ppr <- pdf_text("assets/wq_burning.pdf")
info <- pdf_info("assets/wq_burning.pdf")
ppr_txt <- data.frame(line = 1:8, text = ppr)
View(ppr_txt)
sbj <- unlist(strsplit(info$keys$Subject, " "))
dmn <- unlist(strsplit(info$keys$`CrossMarkDomains[1]`," "))
ctr <- unlist(strsplit(info$keys$Creator, " "))
# doi <- unlist(strsplit(info$keys$doi, " "))
# Cleaning pdf file
ppr_txt$text <- sub(paste(sbj, collapse = '|'),'',ppr_txt)
ppr_txt$text <- sub(paste(dmn, collapse = '|'),'',ppr_txt)
ppr_txt$text <- sub(paste(ctr, collapse = '|'),'',ppr_txt)
View(ppr_txt)
gc()
gc()
###############################################################################
#Text Analysis of a Research Article
###############################################################################
# Setting up libraries
require(librarian)
librarian::shelf(dplyr, tidytext, tidyverse,stringr,stringi,
widyr,igraph, ggraph,
wordcloud, reshape2, graphlayouts,
pluralize, quanteda, qgraph, cowplot, readr, pdftools)
# # Downloading the file
# download.file("https://www.nature.com/articles/s41467-021-22747-3.pdf",
#               destfile = "~/GitHub/burn_iss-website/assets/wq_burning1.pdf", mode = "wb")
# Reading our file
ppr <- pdf_text("assets/wq_burning.pdf")
info <- pdf_info("assets/wq_burning.pdf")
ppr_txt <- data.frame(line = 1:8, text = ppr)
# Extracting pdf metadata and removing it from text body
sbj <- unlist(strsplit(info$keys$Subject, " "))
dmn <- unlist(strsplit(info$keys$`CrossMarkDomains[1]`," "))
ctr <- unlist(strsplit(info$keys$Creator, " "))
# doi <- unlist(strsplit(info$keys$doi, " "))
# cdm <- unlist(strsplit(info$keys$`CrossMarkDomains[2]`," "))
# Cleaning pdf file
ppr_txt$text <- sub(paste(sbj, collapse = '|'),'',ppr_txt)
ppr_txt$text <- sub(paste(dmn, collapse = '|'),'',ppr_txt)
ppr_txt$text <- sub(paste(ctr, collapse = '|'),'',ppr_txt)
# ppr_txt$text <- sub(paste(doi, collapse = '|'),'',ppr_txt)#long exec time
# ppr_txt$text <- sub(paste(cdm, collapse = '|'),'',ppr_txt)#long exec time
ppr_cln <- ppr_txt %>%
str_to_lower()
ppr_cln <- ppr_txt %>%
stri_trans_tolower()
ppr_cln <- ppr_txt %>%
stri_trans_tolower(ppr_txt$text)
typeof(ppr_txt)
# Reading our file
ppr <- pdf_text("assets/wq_burning.pdf")
info <- pdf_info("assets/wq_burning.pdf")
ppr_txt <- unlist(ppr)
ppr <- pdf_text("assets/wq_burning.pdf")
info <- pdf_info("assets/wq_burning.pdf")
ppr_txt0 <- unlist(ppr)
ppr_txt <- data.frame(line = 1:8, text = ppr_txt0)
View(ppr_txt)
sbj <- unlist(strsplit(info$keys$Subject, " "))
dmn <- unlist(strsplit(info$keys$`CrossMarkDomains[1]`," "))
ctr <- unlist(strsplit(info$keys$Creator, " "))
# Cleaning pdf file
ppr_txt$text <- sub(paste(sbj, collapse = '|'),'',ppr_txt)
ppr_txt$text <- sub(paste(dmn, collapse = '|'),'',ppr_txt)
ppr_txt$text <- sub(paste(ctr, collapse = '|'),'',ppr_txt)
ppr_cln <- ppr_txt %>%
stri_trans_tolower(ppr_txt$text)
ppr_cln <- ppr_txt%>%
unnest_tokens(output = word, input = text) %>%
anti_join(stop_words, by = "word") %>%
filter(str_detect(word,"[:alpha:]")) %>%
count(word, sort = TRUE) %>%
mutate(length = nchar(word)) %>%
select(word, n) %>%
mutate(rank = row_number(),
total=sum(n),
t_freq = n/total)
ppr_cln %>% with(wordcloud(word,n,max.words = 400))
info$keys$doi
sbj <- unlist(strsplit(info$keys$Subject, " "))
dmn <- unlist(strsplit(info$keys$`CrossMarkDomains[1]`," "))
ctr <- unlist(strsplit(info$keys$Creator, " "))
doi <- info$keys$doi
cdm <- info$keys$`CrossMarkDomains[2]`
# Cleaning pdf file
ppr_txt$text <- sub(paste(sbj, collapse = '|'),'',ppr_txt)
ppr_txt$text <- sub(paste(dmn, collapse = '|'),'',ppr_txt)
paste(doi)
View(ppr_cln)
###############################################################################
#Text Analysis of a Research Article
###############################################################################
# Setting up libraries
require(librarian)
librarian::shelf(dplyr, tidytext, tidyverse,stringr,stringi,
widyr,igraph, ggraph,
wordcloud, reshape2, graphlayouts,
pluralize, quanteda, qgraph, cowplot, readr, pdftools)
updateR()
require(installr)
updateR()
# Setting up libraries
require(librarian)
librarian::shelf(dplyr, tidytext, tidyverse,stringr,stringi,
widyr,igraph, ggraph,
wordcloud, reshape2, graphlayouts,
pluralize, quanteda, qgraph, cowplot, readr, pdftools)
# Reading our file
ppr <- pdf_text("assets/wq_burning.pdf")
info <- pdf_info("assets/wq_burning.pdf")
ppr_txt0 <- unlist(ppr)
ppr_txt <- data.frame(line = 1:8, text = ppr_txt0)
sbj <- unlist(strsplit(info$keys$Subject, " "))
dmn <- unlist(strsplit(info$keys$`CrossMarkDomains[1]`," "))
ctr <- unlist(strsplit(info$keys$Creator, " "))
doi <- info$keys$doi
cdm <- info$keys$`CrossMarkDomains[2]`
ppr_txt$text <- sub(paste(sbj, collapse = '|'),'',ppr_txt)
ppr_txt$text <- sub(paste(dmn, collapse = '|'),'',ppr_txt)
ppr_txt$text <- sub(paste(ctr, collapse = '|'),'',ppr_txt)
ppr_cln <- ppr_txt%>%
unnest_tokens(output = word, input = text) %>%
anti_join(stop_words, by = "word") %>%
filter(str_detect(word,"[:alpha:]")) %>%
count(word, sort = TRUE) %>%
mutate(length = nchar(word)) %>%
select(word, n) %>%
mutate(rank = row_number(),
total=sum(n),
t_freq = n/total)
ppr_cln %>% with(wordcloud(word,n,max.words = 400))
