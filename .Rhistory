# Cleaning pdf file
ppr_txt$text <- sub(paste(sbj, collapse = '|'),'',ppr_txt)
ppr_txt$text <- sub(paste(dmn, collapse = '|'),'',ppr_txt)
ppr_txt$text <- sub(paste(ctr, collapse = '|'),'',ppr_txt)
ppr_cln <- as.data.frame(ppr_txt %>%
str_to_lower() %>%
str_replace_all("\n"," ")
ppr_cln <- as.data.frame(ppr_txt %>%
View(ppr_txt)
###############################################################################
#Text Analysis of a Research Article
###############################################################################
# Setting up libraries
require(librarian)
librarian::shelf(dplyr, tidytext, tidyverse,stringr,stringi,
widyr,igraph, ggraph,
wordcloud, reshape2, graphlayouts,
pluralize, quanteda, qgraph, cowplot, readr, pdftools)
# # Downloading the file
# download.file("https://www.nature.com/articles/s41467-021-22747-3.pdf",
#               destfile = "~/GitHub/burn_iss-website/assets/wq_burning1.pdf", mode = "wb")
# Reading our file
ppr <- pdf_text("assets/wq_burning1.pdf")
# Reading our file
ppr <- pdf_text("assets/wq_burning.pdf")
info <- pdf_info("assets/wq_burning.pdf")
ppr_txt <- data.frame(line = 1:8, text = ppr)
View(ppr_txt)
sbj <- unlist(strsplit(info$keys$Subject, " "))
dmn <- unlist(strsplit(info$keys$`CrossMarkDomains[1]`," "))
ctr <- unlist(strsplit(info$keys$Creator, " "))
# doi <- unlist(strsplit(info$keys$doi, " "))
# Cleaning pdf file
ppr_txt$text <- sub(paste(sbj, collapse = '|'),'',ppr_txt)
ppr_txt$text <- sub(paste(dmn, collapse = '|'),'',ppr_txt)
ppr_txt$text <- sub(paste(ctr, collapse = '|'),'',ppr_txt)
View(ppr_txt)
gc()
gc()
###############################################################################
#Text Analysis of a Research Article
###############################################################################
# Setting up libraries
require(librarian)
librarian::shelf(dplyr, tidytext, tidyverse,stringr,stringi,
widyr,igraph, ggraph,
wordcloud, reshape2, graphlayouts,
pluralize, quanteda, qgraph, cowplot, readr, pdftools)
# # Downloading the file
# download.file("https://www.nature.com/articles/s41467-021-22747-3.pdf",
#               destfile = "~/GitHub/burn_iss-website/assets/wq_burning1.pdf", mode = "wb")
# Reading our file
ppr <- pdf_text("assets/wq_burning.pdf")
info <- pdf_info("assets/wq_burning.pdf")
ppr_txt <- data.frame(line = 1:8, text = ppr)
# Extracting pdf metadata and removing it from text body
sbj <- unlist(strsplit(info$keys$Subject, " "))
dmn <- unlist(strsplit(info$keys$`CrossMarkDomains[1]`," "))
ctr <- unlist(strsplit(info$keys$Creator, " "))
# doi <- unlist(strsplit(info$keys$doi, " "))
# cdm <- unlist(strsplit(info$keys$`CrossMarkDomains[2]`," "))
# Cleaning pdf file
ppr_txt$text <- sub(paste(sbj, collapse = '|'),'',ppr_txt)
ppr_txt$text <- sub(paste(dmn, collapse = '|'),'',ppr_txt)
ppr_txt$text <- sub(paste(ctr, collapse = '|'),'',ppr_txt)
# ppr_txt$text <- sub(paste(doi, collapse = '|'),'',ppr_txt)#long exec time
# ppr_txt$text <- sub(paste(cdm, collapse = '|'),'',ppr_txt)#long exec time
ppr_cln <- ppr_txt %>%
str_to_lower()
ppr_cln <- ppr_txt %>%
stri_trans_tolower()
ppr_cln <- ppr_txt %>%
stri_trans_tolower(ppr_txt$text)
typeof(ppr_txt)
# Reading our file
ppr <- pdf_text("assets/wq_burning.pdf")
info <- pdf_info("assets/wq_burning.pdf")
ppr_txt <- unlist(ppr)
ppr <- pdf_text("assets/wq_burning.pdf")
info <- pdf_info("assets/wq_burning.pdf")
ppr_txt0 <- unlist(ppr)
ppr_txt <- data.frame(line = 1:8, text = ppr_txt0)
View(ppr_txt)
sbj <- unlist(strsplit(info$keys$Subject, " "))
dmn <- unlist(strsplit(info$keys$`CrossMarkDomains[1]`," "))
ctr <- unlist(strsplit(info$keys$Creator, " "))
# Cleaning pdf file
ppr_txt$text <- sub(paste(sbj, collapse = '|'),'',ppr_txt)
ppr_txt$text <- sub(paste(dmn, collapse = '|'),'',ppr_txt)
ppr_txt$text <- sub(paste(ctr, collapse = '|'),'',ppr_txt)
ppr_cln <- ppr_txt %>%
stri_trans_tolower(ppr_txt$text)
ppr_cln <- ppr_txt%>%
unnest_tokens(output = word, input = text) %>%
anti_join(stop_words, by = "word") %>%
filter(str_detect(word,"[:alpha:]")) %>%
count(word, sort = TRUE) %>%
mutate(length = nchar(word)) %>%
select(word, n) %>%
mutate(rank = row_number(),
total=sum(n),
t_freq = n/total)
ppr_cln %>% with(wordcloud(word,n,max.words = 400))
info$keys$doi
sbj <- unlist(strsplit(info$keys$Subject, " "))
dmn <- unlist(strsplit(info$keys$`CrossMarkDomains[1]`," "))
ctr <- unlist(strsplit(info$keys$Creator, " "))
doi <- info$keys$doi
cdm <- info$keys$`CrossMarkDomains[2]`
# Cleaning pdf file
ppr_txt$text <- sub(paste(sbj, collapse = '|'),'',ppr_txt)
ppr_txt$text <- sub(paste(dmn, collapse = '|'),'',ppr_txt)
paste(doi)
View(ppr_cln)
###############################################################################
#Text Analysis of a Research Article
###############################################################################
# Setting up libraries
require(librarian)
librarian::shelf(dplyr, tidytext, tidyverse,stringr,stringi,
widyr,igraph, ggraph,
wordcloud, reshape2, graphlayouts,
pluralize, quanteda, qgraph, cowplot, readr, pdftools)
updateR()
require(installr)
updateR()
# Setting up libraries
require(librarian)
librarian::shelf(dplyr, tidytext, tidyverse,stringr,stringi,
widyr,igraph, ggraph,
wordcloud, reshape2, graphlayouts,
pluralize, quanteda, qgraph, cowplot, readr, pdftools)
# Reading our file
ppr <- pdf_text("assets/wq_burning.pdf")
info <- pdf_info("assets/wq_burning.pdf")
ppr_txt0 <- unlist(ppr)
ppr_txt <- data.frame(line = 1:8, text = ppr_txt0)
sbj <- unlist(strsplit(info$keys$Subject, " "))
dmn <- unlist(strsplit(info$keys$`CrossMarkDomains[1]`," "))
ctr <- unlist(strsplit(info$keys$Creator, " "))
doi <- info$keys$doi
cdm <- info$keys$`CrossMarkDomains[2]`
ppr_txt$text <- sub(paste(sbj, collapse = '|'),'',ppr_txt)
ppr_txt$text <- sub(paste(dmn, collapse = '|'),'',ppr_txt)
ppr_txt$text <- sub(paste(ctr, collapse = '|'),'',ppr_txt)
ppr_cln <- ppr_txt%>%
unnest_tokens(output = word, input = text) %>%
anti_join(stop_words, by = "word") %>%
filter(str_detect(word,"[:alpha:]")) %>%
count(word, sort = TRUE) %>%
mutate(length = nchar(word)) %>%
select(word, n) %>%
mutate(rank = row_number(),
total=sum(n),
t_freq = n/total)
ppr_cln %>% with(wordcloud(word,n,max.words = 400))
knitr::opts_chunk$set(echo = TRUE)
#library(dplyr)
#library(tidytext)
#library(tidyverse)
#library(widyr)
#library(igraph)
#library(ggraph)
#library(wordcloud)
#library(reshape2)
#library(graphlayouts)
#library(pluralize)
#library(quanteda)
# library(spacyr)
# library(reticulate)
#library(igraph)
#library(graphlayouts)
#library(qgraph)
# require(pacman)
# pacman::p_load(dplyr, tidytext, tidyverse,
#             widyr,igraph, ggraph,
#             wordcloud, reshape2, graphlayouts,
#             pluralize, quanteda, qgraph, cowplot, readr)
require(librarian)
librarian::shelf(plyr, tidytext, tidyverse,
widyr,igraph, ggraph,
wordcloud, reshape2, graphlayouts,
pluralize, quanteda, qgraph, cowplot, readr)
#Function to adjust table width:
html_table_width <- function(kable_output, width){
width_html <- paste0(paste0('<col width="', width, '">'), collapse = "\n")
sub("<table>", paste0("<table>\n", width_html), kable_output)
}
# Source: https://github.com/rstudio/bookdown/issues/122
knitr::opts_chunk$set(echo = TRUE)
require(librarian)
librarian::shelf(plyr, tidytext, tidyverse,
widyr,igraph, ggraph,
wordcloud, reshape2, graphlayouts,
pluralize, quanteda, qgraph, cowplot, readr)
#Function to adjust table width:
html_table_width <- function(kable_output, width){
width_html <- paste0(paste0('<col width="', width, '">'), collapse = "\n")
sub("<table>", paste0("<table>\n", width_html), kable_output)
}
# Source: https://github.com/rstudio/bookdown/issues/122
abs_df <- read_csv("assets/data/fire_science_future.csv",show_col_types = FALSE)
glimpse(abs_df)
#
# t_df1 <- as.data.frame(rbind(t_df[c(1:3),],t_df[c(68:71),], row.names = FALSE))
# colnames(t_df1) <- c("ID", "Question type", "Answers")
# knitr::kable(head(t_df1),format="html")%>%
#    html_table_width(c(100,100,500))
abs_df <- read_csv("assets/data/fire_science_future.csv",show_col_types = FALSE)
glimpse(abs_df)
abs_df1 <- dplyr:select(abs_df,Key,Author,Title,Journal,`Abstract Note`)
require(librarian)
librarian::shelf(dplyr, plyr, tidytext, tidyverse,
widyr,igraph, ggraph,
wordcloud, reshape2, graphlayouts,
pluralize, quanteda, qgraph, cowplot, readr)
#Function to adjust table width:
html_table_width <- function(kable_output, width){
width_html <- paste0(paste0('<col width="', width, '">'), collapse = "\n")
sub("<table>", paste0("<table>\n", width_html), kable_output)
}
# Source: https://github.com/rstudio/bookdown/issues/122
abs_df <- read_csv("assets/data/fire_science_future.csv",show_col_types = FALSE)
glimpse(abs_df)
abs_df1 <- dplyr:select(abs_df,Key,Author,Title,Journal,`Abstract Note`)
abs_df <- read_csv("assets/data/fire_science_future.csv",show_col_types = FALSE)
glimpse(abs_df)
abs_df1 <- dplyr::select(abs_df,Key,Author,Title,Journal,`Abstract Note`)
abs_df <- read_csv("assets/data/fire_science_future.csv",show_col_types = FALSE)
glimpse(abs_df)
abs_df1 <- dplyr::select(abs_df,Key,Author,Title,`Publication Title`,`Abstract Note`)
#
# t_df1 <- as.data.frame(rbind(t_df[c(1:3),],t_df[c(68:71),], row.names = FALSE))
# colnames(t_df1) <- c("ID", "Question type", "Answers")
# knitr::kable(head(t_df1),format="html")%>%
#    html_table_width(c(100,100,500))
glimpse(abs_df1)
abs_df <- read_csv("assets/data/fire_science_future.csv",show_col_types = FALSE)
glimpse(abs_df)
abs_df1 <- dplyr::select(abs_df,Key,Author,Title,`Publication Title`,`Abstract Note`)
abs_df1 <- rename(abs_df1,
id = key,
author = Author,
title = Title,
journal = `Publication Title`,
abstract = `Abstract Note)
#
# t_df1 <- as.data.frame(rbind(t_df[c(1:3),],t_df[c(68:71),], row.names = FALSE))
# colnames(t_df1) <- c("ID", "Question type", "Answers")
# knitr::kable(head(t_df1),format="html")%>%
#    html_table_width(c(100,100,500))
glimpse(abs_df1)
abs_df <- read_csv("assets/data/fire_science_future.csv",show_col_types = FALSE)
glimpse(abs_df)
abs_df1 <- dplyr::select(abs_df,Key,Author,Title,`Publication Title`,`Abstract Note`)
abs_df1 <- rename(abs_df1,
id = key,
author = Author,
title = Title,
journal = `Publication Title`,
abstract = `Abstract Note`)
abs_df <- read_csv("assets/data/fire_science_future.csv",show_col_types = FALSE)
glimpse(abs_df)
abs_df1 <- dplyr::select(abs_df,Key,Author,Title,`Publication Title`,`Abstract Note`)
abs_df1 <- rename(abs_df1,
id = key,
author = Author,
title = Title,
journal = `Publication Title`,
abstract = `Abstract Note`)
abs_df <- read_csv("assets/data/fire_science_future.csv",show_col_types = FALSE)
glimpse(abs_df)
abs_df1 <- dplyr::select(abs_df,Key,Author,Title,`Publication Title`,`Abstract Note`)
abs_df1 <- rename(abs_df1,
id = Key,
author = Author,
title = Title,
journal = `Publication Title`,
abstract = `Abstract Note`)
#
# t_df1 <- as.data.frame(rbind(t_df[c(1:3),],t_df[c(68:71),], row.names = FALSE))
# colnames(t_df1) <- c("ID", "Question type", "Answers")
# knitr::kable(head(t_df1),format="html")%>%
#    html_table_width(c(100,100,500))
glimpse(abs_df1)
abs_df <- read_csv("assets/data/fire_science_future.csv",show_col_types = FALSE)
abs_df1 <- dplyr::select(abs_df,Key,Author,Title,`Publication Title`,`Abstract Note`)
abs_df1 <- rename(abs_df1,
id = Key,
author = Author,
title = Title,
journal = `Publication Title`,
abstract = `Abstract Note`)
abs_df1%>%
unnest_tokens(output = word, input = title)%>%
count(word, sort = TRUE)%>%
distinct() %>%
with(wordcloud(word,n, scale = c(3, .07)))
abs_df1%>%
unnest_tokens(output = word, input = title)%>%
anti_join(stop_words, by = "word")%>%
filter(str_detect(word,"[:alpha:]"))%>%
count(word, sort = TRUE)%>%
distinct() %>%
with(wordcloud(word,n, scale = c(3, .07)))
abs_df1$title
abs_df1%>%
unnest_tokens(output = word, input = title)%>%
anti_join(stop_words, by = "word")%>%
filter(str_detect(word,"[:alpha:]"))%>%
count(word, sort = TRUE)%>%
distinct() %>%
with(wordcloud(word,n))#, scale = c(3, .07)
unnest_tokens(output = word, input = title)
abs_df1%>%
unnest_tokens(output = word, input = title)
abs_df1%>% select(id,title) %>%
unnest_tokens(output = word, input = title)
abs_df1%>% select(id,title) %>%
unnest_tokens(output = word, input = title)%>%
anti_join(stop_words, by = "word")%>%
filter(str_detect(word,"[:alpha:]"))%>%
count(word, sort = TRUE)%>%
distinct() %>%
with(wordcloud(word,n))#, scale = c(3, .07)
abs_df1%>% select(id,title) %>%
unnest_tokens(output = word, input = title)%>%
anti_join(stop_words, by = "word")%>%
filter(str_detect(word,"[:alpha:]"))%>%
count(word, sort = TRUE)%>%
distinct()
abs_df1%>% select(id,title) %>%
unnest_tokens(output = word, input = title)%>%
anti_join(stop_words, by = "word")%>%
filter(str_detect(word,"[:alpha:]"))%>%
count(word, sort = TRUE)%>%
distinct() %>%
with(wordcloud(word,n,scale = c(1.5, .07)))#,
?wordcloud
abs_df1%>% select(id,title) %>%
unnest_tokens(output = word, input = title)%>%
anti_join(stop_words, by = "word")%>%
filter(str_detect(word,"[:alpha:]"))%>%
count(word, sort = TRUE)%>%
distinct() %>%
with(wordcloud(word,n,scale = c(3, .05)))#,
abs_df1%>% select(id,title) %>%
unnest_tokens(output = word, input = title)%>%
anti_join(stop_words, by = "word")%>%
filter(str_detect(word,"[:alpha:]"))%>%
count(word, sort = TRUE)%>%
distinct() %>%
with(wordcloud(word,n,scale = c(3, .05),max.words = Inf))#,
abs_df1%>% select(id,title) %>%
unnest_tokens(output = word, input = title)%>%
anti_join(stop_words, by = "word")%>%
filter(str_detect(word,"[:alpha:]"))%>%
count(word, sort = TRUE)%>%
distinct() %>%
with(wordcloud(word,n,scale = c(3, .05),max.words = Inf,min.freq=1))#,
abs_df1%>% select(id,title) %>%
unnest_tokens(output = word, input = title)%>%
anti_join(stop_words, by = "word")%>%
filter(str_detect(word,"[:alpha:]"))%>%
count(word, sort = TRUE)%>%
distinct() %>%
with(wordcloud(word,n,scale = c(4, .05),max.words = Inf,min.freq=1))#,
abs_df1%>% select(id,title) %>%
unnest_tokens(output = word, input = title)%>%
anti_join(stop_words, by = "word")%>%
filter(str_detect(word,"[:alpha:]"))%>%
count(word, sort = TRUE)%>%
distinct() %>%
with(wordcloud(word,n,scale = c(4, .05),max.words = Inf,min.freq=2))#,
abs_df1%>% select(id,title) %>%
unnest_tokens(output = word, input = title)%>%
anti_join(stop_words, by = "word")%>%
filter(str_detect(word,"[:alpha:]"))%>%
count(word, sort = TRUE)%>%
distinct() %>%
with(wordcloud(word,n,scale = c(4, .07),max.words = Inf,min.freq=1))#,
abs_df1%>% select(id,title) %>%
unnest_tokens(output = word, input = title)%>%
anti_join(stop_words, by = "word")%>%
filter(str_detect(word,"[:alpha:]"))%>%
count(word, sort = TRUE)%>%
distinct() %>%
filter(word!=fire) %>%
with(wordcloud(word,n,scale = c(4, .07),max.words = Inf,min.freq=1))#,
abs_df1%>% select(id,title) %>%
unnest_tokens(output = word, input = title)%>%
anti_join(stop_words, by = "word")%>%
filter(str_detect(word,"[:alpha:]"))%>%
count(word, sort = TRUE)%>%
distinct() %>%
filter(word!="fire") %>%
with(wordcloud(word,n,scale = c(4, .07),max.words = Inf,min.freq=1))#,
abs_df1%>% select(id,title) %>%
unnest_tokens(output = word, input = title)%>%
anti_join(stop_words, by = "word")%>%
filter(str_detect(word,"[:alpha:]"))%>%
count(word, sort = TRUE)%>%
distinct() %>%
filter(word!="fire" | word!="wildfire") %>%
with(wordcloud(word,n,scale = c(4, .07),max.words = Inf,min.freq=1))
abs_df1%>% select(id,title) %>%
unnest_tokens(output = word, input = title)%>%
anti_join(stop_words, by = "word")%>%
filter(str_detect(word,"[:alpha:]"))%>%
count(word, sort = TRUE)%>%
distinct() %>%
filter(word!="fire") %>%
filter(word!= "Wildfire") %>%
with(wordcloud(word,n,scale = c(4, .07),max.words = Inf,min.freq=1))
abs_df1%>% select(id,title) %>%
unnest_tokens(output = word, input = title)%>%
anti_join(stop_words, by = "word")%>%
filter(str_detect(word,"[:alpha:]"))%>%
count(word, sort = TRUE)%>%
distinct() %>%
filter(word!="fire") %>%
filter(word!= "Wildfire") %>%
with(wordcloud(word,n,scale = c(4, .07),max.words = Inf,min.freq=1))
abs_df1%>% select(id,title) %>%
unnest_tokens(output = word, input = title)%>%
anti_join(stop_words, by = "word")%>%
filter(str_detect(word,"[:alpha:]"))%>%
count(word, sort = TRUE)%>%
distinct() %>%
# filter(word!="fire") %>%
# filter(word!= "Wildfire") %>%
with(wordcloud(word,n,scale = c(4, .07),max.words = Inf,min.freq=1))
abs_df1%>% select(id,title) %>%
unnest_tokens(output = word, input = title)%>%
anti_join(stop_words, by = "word")%>%
filter(str_detect(word,"[:alpha:]"))%>%
count(word, sort = TRUE)%>%
distinct() %>%
# filter(word!="fire") %>%
# filter(word!= "Wildfire") %>%
with(wordcloud(word,n,scale = c(4, .07),max.words = Inf,min.freq=1))
knitr::opts_chunk$set(echo = TRUE)
require(librarian)
librarian::shelf(dplyr, plyr, tidytext, tidyverse,
widyr,igraph, ggraph,
wordcloud, reshape2, graphlayouts,
pluralize, quanteda, qgraph, cowplot, readr)
#Function to adjust table width:
html_table_width <- function(kable_output, width){
width_html <- paste0(paste0('<col width="', width, '">'), collapse = "\n")
sub("<table>", paste0("<table>\n", width_html), kable_output)
}
# Source: https://github.com/rstudio/bookdown/issues/122
abs_df <- read_csv("assets/data/fire_science_future.csv",show_col_types = FALSE)
abs_df1 <- dplyr::select(abs_df,Key,Author,Title,`Publication Title`,`Abstract Note`)
abs_df1 <- rename(abs_df1,
id = Key,
author = Author,
title = Title,
journal = `Publication Title`,
abstract = `Abstract Note`)
abs_df <- read_csv("assets/data/fire_science_future.csv",show_col_types = FALSE)
abs_df1 <- dplyr::select(abs_df,Key,Author,Title,`Publication Title`,`Abstract Note`)
abs_df1 <- rename(abs_df1,
id = Key,
author = Author,
title = Title,
journal = `Publication Title`,
abstract = `Abstract Note`)
abs_df1%>% select(id,title) %>%
unnest_tokens(output = word, input = title)%>%
anti_join(stop_words, by = "word")%>%
filter(str_detect(word,"[:alpha:]"))%>%
count(word, sort = TRUE)%>%
distinct() %>%
# filter(word!="fire") %>%
# filter(word!= "Wildfire") %>%
with(wordcloud(word,n,scale = c(4, .07),max.words = Inf,min.freq=1))
abs_df <- read_csv("assets/data/fire_science_future.csv",show_col_types = FALSE)
abs_df1 <- dplyr::select(abs_df,Key,Author,Title,`Publication Title`,`Abstract Note`)
abs_df1
glimpse(abs_df1)
abs_df1 <- rename(abs_df1,
id = Key,
author = Author,
title = Title,
journal = `Publication Title`,
abstract = `Abstract Note`)
abs_df <- read_csv("assets/data/fire_science_future.csv",show_col_types = FALSE)
abs_df1 <- dplyr::select(abs_df,Key,Author,Title,`Publication Title`,`Abstract Note`)
abs_df1 <- rename(abs_df1,
id = Key,
author = Author,
title = Title,
journal = `Publication Title`,
abstract = `Abstract Note`)
abs_df <- read_csv("assets/data/fire_science_future.csv",show_col_types = TRUE)
glimpse(abs_df)
abs_df1 <- dplyr::select(abs_df,Key,Author,Title,`Publication Title`,`Abstract Note`)
abs_df1 <- rename(abs_df1,
id = Key,
author = Author,
title = Title,
journal = `Publication Title`,
abstract = `Abstract Note`)
knitr::opts_chunk$set(echo = TRUE)
require(librarian)
librarian::shelf(dplyr, tidytext, tidyverse,
widyr,igraph, ggraph,
wordcloud, reshape2, graphlayouts,
pluralize, quanteda, qgraph, cowplot, readr,spacyr)
spacy_install()
