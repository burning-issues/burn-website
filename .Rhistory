#library(widyr)
#library(igraph)
#library(ggraph)
#library(wordcloud)
#library(reshape2)
#library(graphlayouts)
#library(pluralize)
#library(quanteda)
# library(spacyr)
# library(reticulate)
#library(igraph)
#library(graphlayouts)
#library(qgraph)
require(pacman)
pacman::p_load(dplyr, tidytext, tidyverse,
widyr,igraph, ggraph,
wordcloud, reshape2, graphlayouts,
pluralize, quanteda, qgraph, cowplot, readr)
#Function to adjust table width:
html_table_width <- function(kable_output, width){
width_html <- paste0(paste0('<col width="', width, '">'), collapse = "\n")
sub("<table>", paste0("<table>\n", width_html), kable_output)
}
# Source: https://github.com/rstudio/bookdown/issues/122
# setwd("../community-paper/wildfires_watersheds_vault")
t_df <- readr("assets/data/wildfires-survey-answers_formatted.csv")
# setwd("../community-paper/wildfires_watersheds_vault")
t_df <- read_csv("assets/data/wildfires-survey-answers_formatted.csv")
# t_df1 <- as.data.frame(rbind(t_df[c(1:3),],t_df[c(68:71),], row.names = FALSE))
# colnames(t_df1) <- c("ID", "Question type", "Answers")
# knitr::kable(head(t_df1),format="html")%>%
#    html_table_width(c(100,100,100))
# setwd("../community-paper/wildfires_watersheds_rdocs")
head(t_df)
# setwd("../community-paper/wildfires_watersheds_vault")
t_df <- read_csv("assets/data/wildfires-survey-answers_formatted.csv")
t_df1 <- as.data.frame(rbind(t_df[c(1:3),],t_df[c(68:71),], row.names = FALSE))
# colnames(t_df1) <- c("ID", "Question type", "Answers")
# knitr::kable(head(t_df1),format="html")%>%
#    html_table_width(c(100,100,100))
# setwd("../community-paper/wildfires_watersheds_rdocs")
# setwd("../community-paper/wildfires_watersheds_vault")
t_df <- read_csv("assets/data/wildfires-survey-answers_formatted.csv",show_col_types = FALSE)
t_df1 <- as.data.frame(rbind(t_df[c(1:3),],t_df[c(68:71),], row.names = FALSE))
# colnames(t_df1) <- c("ID", "Question type", "Answers")
# knitr::kable(head(t_df1),format="html")%>%
#    html_table_width(c(100,100,100))
# setwd("../community-paper/wildfires_watersheds_rdocs")
t_df <- read_csv("assets/data/wildfires-survey-answers_formatted.csv",show_col_types = FALSE)
t_df1 <- as.data.frame(rbind(t_df[c(1:3),],t_df[c(68:71),], row.names = FALSE))
colnames(t_df1) <- c("ID", "Question type", "Answers")
knitr::kable(head(t_df1),format="html")%>%
html_table_width(c(100,100,100))
t_df <- read_csv("assets/data/wildfires-survey-answers_formatted.csv",show_col_types = FALSE)
t_df1 <- as.data.frame(rbind(t_df[c(1:3),],t_df[c(68:71),], row.names = FALSE))
colnames(t_df1) <- c("ID", "Question type", "Answers")
knitr::kable(head(t_df1),format="html")%>%
html_table_width(c(100,100,300))
t_df <- read_csv("assets/data/wildfires-survey-answers_formatted.csv",show_col_types = FALSE)
t_df1 <- as.data.frame(rbind(t_df[c(1:3),],t_df[c(68:71),], row.names = FALSE))
colnames(t_df1) <- c("ID", "Question type", "Answers")
knitr::kable(head(t_df1),format="html")%>%
html_table_width(c(100,100,400))
t_df <- read_csv("assets/data/wildfires-survey-answers_formatted.csv",show_col_types = FALSE)
t_df1 <- as.data.frame(rbind(t_df[c(1:3),],t_df[c(68:71),], row.names = FALSE))
colnames(t_df1) <- c("ID", "Question type", "Answers")
knitr::kable(head(t_df1),format="html")%>%
html_table_width(c(100,100,500))
#For research area questions
ra_dat <- t_df%>%filter(question=="research-a")
#For pressing questions
pq_dat <- t_df%>%filter(question=="pressing-q")
ra_dat%>%
unnest_tokens(output = word, input = answers)%>%
count(word, sort = TRUE)%>%
distinct() %>%
with(wordcloud(word,n, scale = c(3, .07)))
stp_eg <- as.data.frame(rbind(head(stop_words),
tail(stop_words)),
row.names = FALSE)
colnames(stp_eg) <- c("Stop word", "Lexicon")
knitr::kable(stp_eg,format="html") %>%
html_table_width(c(200,200))
pq_tokens <- pq_dat %>%
unnest_tokens(output = word, input = answers)%>%
anti_join(stop_words, by = "word")%>%
filter(str_detect(word,"[:alpha:]"))%>%
distinct() %>%
count(word, sort = FALSE) %>%
mutate(length = nchar(word))
head(pq_tokens)
pq_dat %>%
unnest_tokens(output = word, input = answers)%>%
distinct() %>%
count(word, sort = TRUE) %>%
with(wordcloud(word,n, scale = c(4, .1)))
gls <- as.data.frame(pq_dat %>%
unnest_tokens(output = word, input = answers)%>%
anti_join(stop_words, by = "word")%>%
filter(str_detect(word,"[:alpha:]"))%>%
distinct() %>%
count(word, sort = TRUE))
head(gls)
pq_dat %>%
unnest_tokens(output = word, input = answers)%>%
anti_join(stop_words, by = "word")%>%
filter(str_detect(word,"[:alpha:]"))%>%
distinct() %>%
count(word, sort = TRUE) %>%
with(wordcloud(word,n, scale = c(4, .1)))
gls <- as.data.frame(pq_dat %>%
unnest_tokens(output = word, input = answers)%>%
anti_join(stop_words, by = "word")%>%
filter(str_detect(word,"[:alpha:]"))%>%
distinct() %>%
count(word, sort = TRUE))
gls_dsp <- rbind(head(gls),tail(gls))
colnames(gls_dsp) <- c("Word", "Frequency")
knitr::kable(gls_dsp,format="html") %>%
html_table_width(c(200,200))
gls_fw <- filter(gls, n > 2)
gls_fwp <- rbind(head(gls_fw),tail(gls_fw))
colnames(gls_fwp) <- c("Word", "Frequency")
knitr::kable(gls_fwp,format="html") %>%
html_table_width(c(200,200))
eda_words <- pq_dat %>%
unnest_tokens(output = word, input = answers)%>%
anti_join(stop_words, by = "word")%>%
filter(str_detect(word,"[:alpha:]"))%>%
distinct() %>%
count(word, sort = TRUE) %>%
select(word, n) %>%
mutate(rank = row_number(),
total=sum(n),
t_freq = n/total)
#Distribution of frequency values
eda_words %>% filter(rank<26) %>%
ggplot(aes(t_freq, fct_reorder(word, t_freq), fill = t_freq)) +
geom_col(show.legend = FALSE) +
labs(x = "Frequency", y = NULL)
#Zipf's law for survey answers
eda_words %>%
ggplot(aes(rank,t_freq)) +
geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
geom_abline(intercept = -0.62, slope = -1.1,
color = "gray50", linetype = 2) +
scale_x_log10() +
scale_y_log10()
pq_snt <- pq_dat %>%
unnest_sentences(output = sentences, input = answers)
pq_snt_s <- pq_snt[c(1:6),]
knitr::kable(pq_snt_s,format="html") %>%
html_table_width(c(100,100,400))
###############################################################################
#Text Analysis of a Research Article
###############################################################################
# Setting up libraries
require(librarian)
librarian::shelf(dplyr, tidytext, tidyverse,
widyr,igraph, ggraph,
wordcloud, reshape2, graphlayouts,
pluralize, quanteda, qgraph, cowplot, readr, pdftools)
ppr <- pdf_text("assets/wq_burning1.pdf")
ppr <- pdf_text("assets/wq_burning1.pdf")
ppr_cln <- ppr %>%
str_split("\n") %>%
unlist() %>%
str_to_lower() %>%
str_replace_all("\n"," ") %>%
str_remove_all(",") %>%
str_replace_all("\\s{2,}", " ") %>%
str_replace_all("[:digit:]", " ") %>%
str_replace_all("[:punct:]", " ") %>%
str_trim()
head(ppr_cln)
str(ppr)
head(ppr)
toc <- pdf_toc("assets/wq_burning1.pdf")
View(toc)
jsonlite::toJSON(toc, auto_unbox = TRUE, pretty = TRUE)
info <- pdf_info("assets/wq_burning1.pdf")
info
kwrd <- ppr$keys$keywords
kwrd <- ppr$keys
ppr$keys
View(info)
info$keys
info$keys$Subject
info$keys$Subject
subject <- info$keys$Subject
subject
ppr_cln <- ppr %>%
str_detect(ppr,subject)
ppr_cln <- ppr %>%
str_detect(ppr,subject,negate = FALSE)
subject
paste(subject)
ppr_cln <- ppr %>%
str_detect(ppr,paste(subject),negate = FALSE)
ppr_cln <- ppr %>%
str_detect(ppr,paste(subject, collapse = "|"),negate = FALSE)
subject <- c(info$keys$Subject)
subject
ppr_cln <- ppr %>%
str_detect(ppr,paste(subject),negate = FALSE)
librarian::shelf(dplyr, tidytext, tidyverse,stringr,
widyr,igraph, ggraph,
wordcloud, reshape2, graphlayouts,
pluralize, quanteda, qgraph, cowplot, readr, pdftools)
subject <- c(info$keys$Subject)
ppr_cln <- ppr %>%
str_detect(ppr,paste(subject),negate = FALSE)
subject <- c(info$keys$Subject)
ppr_cln <- ppr %>%
str_detect(ppr,paste(subject),negate = FALSE)
paste(subject)
ppr_cln <- ppr %>%
str_detect(ppr,"Nature Communications, doi:10.1038/s41467-021-22747-3")
ppr_cln <- ppr %>%
str_detect(ppr,"Nature")
unestokens(subject)
unnest_tokens(output = words, input= subject)
unnest_tokens(output = words, input= tbl(subject))
subject
unnest_tokens_(output = words, input= subject)
unnest_tokens(subject,txt)
str(subject)
subject <- as.string(info$keys$Subject)
subject <- info$keys$Subject
str(subject)
paste(subject, collapse = " ")
a <- paste(subject, collapse = " ")
str(a)
a <- paste(subject, collapse = ' ')
a
stra(a)
str(a)
install.packages("stringi")
librarian::shelf(dplyr, tidytext, tidyverse,stringr,stringi,
widyr,igraph, ggraph,
wordcloud, reshape2, graphlayouts,
pluralize, quanteda, qgraph, cowplot, readr, pdftools)
a <- toString(subject)
a
str(a)
# Reading our file
ppr <- pdf_text("assets/wq_burning1.pdf")
info <- pdf_info("assets/wq_burning1.pdf")
ppr_txt <- data.frame(line = 1:8, text = ppr)
subject <- info$keys$Subject
ppr_cln <- ppr_txt %>%
str_to_lower() %>%
unnest_tokens(output = word, input = text)
ppr_cln <- ppr_txt %>%
unnest_tokens(output = word, input = text)
ppr_cln <- ppr_txt %>%
unnest_tokens(output = word, input = text) %>%
filter(str_detect("Nature"))
ppr_cln <- ppr_txt %>%
unnest_tokens(output = word, input = text) %>%
filter(str_detect(word,"Nature"))
ppr_cln <- ppr_txt %>%
unnest_tokens(output = word, input = text) %>%
filter(str_detect(word,paste(subject)))
subject
domain <- info$keys$`CrossMarkDomains[1]`
domain
pages <- info$pages
pages
subject <- info$keys$Subject
domain <- info$keys$`CrossMarkDomains[1]`
creator <- info$keys$Creator
doi <- info$keys$doi
cdomain <- info$keys$`CrossMarkDomains[2]`
ppr_cln <- ppr_txt %>%
unnest_tokens(output = word, input = text) %>%
filter(str_detect(word,paste(subject))) %>%
filter(str_detect(word,paste(domain))) %>%
filter(str_detect(word,paste(creator))) %>%
filter(str_detect(word,paste(doi))) %>%
filter(str_detect(word,paste(cdomain)))
ppr_cln <- ppr_txt %>%
unnest_tokens(output = word, input = text) %>%
filter(str_detect(word,paste(subject))) %>%
filter(str_detect(word,paste(domain))) %>%
filter(str_detect(word,paste(creator))) %>%
filter(str_detect(word,paste(doi))) %>%
filter(str_detect(word,paste(cdomain))) %>%
anti_join(stop_words, by = "word") %>%
filter(str_detect(word,"[:alpha:]")) %>%
count(word, sort = TRUE) %>%
mutate(length = nchar(word)) %>%
select(word, n) %>%
mutate(rank = row_number(),
total=sum(n),
t_freq = n/total)
ppr_cln %>% with(wordcloud(word,n,max.words = 400, scale = c(3.5,0.35)))
ppr_cln %>% with(wordcloud(word,n))
ppr_cln %>% with(wordcloud(word,n,max.words = 400))
View(ppr_cln)
ppr_cln <- ppr_txt %>%
unnest_tokens(output = word, input = text
ppr_cln <- ppr_txt %>%
ppr_cln <- ppr_txt %>%
unnest_tokens(output = word, input = text)
ppr_cln <- ppr_txt %>%
unnest_tokens(output = word, input = text) %>%
filter(str_detect(word,paste(subject)))
str_detect(word,paste(subject)
str_detect(word,paste(subject))
ppr_cln <- ppr_txt %>%
unnest_tokens(output = word, input = text) %>%
str_detect(word,paste(subject))
paste(domain)
str_detect(ppr_cln$word,paste(subject))
str_detect(ppr_cln$word,paste(domain))
str_replace(ppr_cln$word,paste(domain))
str_replace(ppr_cln$word,paste(domain)," ")
str_replace(ppr_cln$word,paste(domain),"hey")
str_detect()
subject
str_split(paste(suject),collapse="")
str_split(paste(suject))
str_split(paste(suject), collapse = "")
###############################################################################
#Text Analysis of a Research Article
###############################################################################
# Setting up libraries
require(librarian)
librarian::shelf(dplyr, tidytext, tidyverse,stringr,stringi,
widyr,igraph, ggraph,
wordcloud, reshape2, graphlayouts,
pluralize, quanteda, qgraph, cowplot, readr, pdftools)
# Reading our file
ppr <- pdf_text("assets/wq_burning1.pdf")
info <- pdf_info("assets/wq_burning1.pdf")
ppr_txt <- data.frame(line = 1:8, text = ppr)
subject <- info$keys$Subject
a <- strsplit(subject," ")
a
unlist(a)
a <- unlist(strsplit(info$keys$Subject," "))
ppr_cln <- ppr_txt %>%
unnest_tokens(output = word, input = text) %>%
str_detect(word,paste(a))
ppr_cln <- ppr_txt %>%
unnest_tokens(output = word, input = text)
paste(a)
a <- strsplit(info$keys$Subject," ")
paste(a)
a <- c(strsplit(info$keys$Subject," "))
a
paste(a)
ppr_cln <- ppr_txt %>%
unnest_tokens(output = word, input = text) %>%
str_detect(word,paste(a))
a <- unlist(c(strsplit(info$keys$Subject," ")))
a
paste(a)
ppr_cln <- ppr_txt %>%
unnest_tokens(output = word, input = text) %>%
str_detect(word,paste(a))
a <- unlist(strsplit(info$keys$Subject," "))
a
b <- str_replace(a," ",",")
paste(b)
ppr_cln <- ppr_txt %>%
unnest_tokens(output = word, input = text) %>%
str_detect(word,paste(b))
a <- unlist(strsplit(info$keys$Subject,"\\."))
a
ppr_cln <- ppr_txt %>%
unnest_tokens(output = word, input = text) %>%
str_detect(word,paste(a))
a <- c(unlist(strsplit(info$keys$Subject,"\\.")))
a
ppr_cln <- ppr_txt %>%
unnest_tokens(output = word, input = text) %>%
str_detect(word,paste(a))
a <- c(unlist(strsplit(info$keys$Subject)))
a <- unlist(strsplit(info$keys$Subject," "))
a
paste(a,collapse = "|")
paste(a,collapse = ",")
ppr_cln <- ppr_txt %>%
unnest_tokens(output = word, input = text) %>%
filter(str_detect(word,paste(a,collapse = "|")),negate = TRUE)
a
a <- as.vector(unlist(strsplit(info$keys$Subject," ")))
a
paste(a)
head(ppr_txt)
ppr_cln <- ppr_txt %>%
unnest_tokens(output = word, input = text)
head(ppr_cln)
filter(str_detect(ppr_txt,paste(a,collapse = "|")),negate = TRUE)
a
paste(a,sep = " ")
# Cleanning up the file
ppr_txt <- ppr_txt %>%
str_replace(paste(a,sep = " "),"")
ppr <- pdf_text("assets/wq_burning1.pdf")
info <- pdf_info("assets/wq_burning1.pdf")
ppr_txt <- data.frame(line = 1:8, text = ppr)
str(ppr_txt)
# Cleanning up the file
ppr_txt1 <- ppr_txt %>%
str_replace(text,paste(a,sep = " "),"")
# Cleanning up the file
ppr_txt1 <- ppr_txt %>%
str_replace(text,paste(a,sep = " "))
# Cleanning up the file
ppr_txt1 <- ppr_txt %>%
str_replace(text,paste(a),"")
# Cleanning up the file
ppr_txt1 <- ppr_txt %>%
str_replace(text,paste(a))
?a
# Cleanning up the file
ppr_txt1 <- ppr_txt %>%
filter(str_detect(text,"Nature"))
ppr_txt1
# Cleanning up the file
ppr_txt1 <- ppr_txt %>%
filter(str_replace(text,"Nature",""))
# Cleanning up the file
ppr_txt1 <- ppr_txt %>%
str_replace(text,"Nature","")
# Cleanning up the file
ppr_txt1 <- ppr_txt %>%
str_replace(text,"Nature"," ")
# Cleanning up the file
ppr_txt1 <- ppr_txt %>%
str_replace(text,"Nature")
# Cleanning up the file
ppr_txt1 <- ppr_txt %>%
str_remove(text,"Nature")
# Cleanning up the file
ppr_txt1 <- ppr_txt %>%
filter(str_remove(text,"Nature"))
b <- strsplit(info$keys$Subject," ")
b
paste(b,collapse="|")
# Cleanning up the file
ppr_txt1 <- ppr_txt %>%
filter(str_remove_all(text,paste(b,collapse = "|"))))
# Cleanning up the file
ppr_txt1 <- ppr_txt %>%
filter(str_remove_all(text,paste(b,collapse = "|")))
ppr_txt$text
# Cleanning up the file
ppr_txt1 <- ppr_txt %>%
filter(str_remove_all(text,paste(b,collapse = "|")))
ppr_txt2 <- ppr_txt %>% str_remove_all(text,paste(b,collapse = "|"))
str_remove_all(ppr_txt$text,paste(b,collapse = "|"))
ppr_txt2 <- ppr_txt %>%
filter(str_detect(text,paste(b,collapse = "|")))
###############################################################################
#Text Analysis of a Research Article
###############################################################################
# Setting up libraries
require(librarian)
librarian::shelf(dplyr, tidytext, tidyverse,stringr,stringi,
widyr,igraph, ggraph,
wordcloud, reshape2, graphlayouts,
pluralize, quanteda, qgraph, cowplot, readr, pdftools)
# Downloading the file
download.file("https://www.nature.com/articles/s41467-021-22747-3.pdf",
destfile = "~/GitHub/burn_iss-website/assets/wq_burning1.pdf", mode = "wb")
# Reading our file
ppr <- pdf_text("assets/wq_burning1.pdf")
#Text Analysis of a Research Article
###############################################################################
# Setting up libraries
require(librarian)
librarian::shelf(dplyr, tidytext, tidyverse,stringr,stringi,
widyr,igraph, ggraph,
wordcloud, reshape2, graphlayouts,
pluralize, quanteda, qgraph, cowplot, readr, pdftools)
# Reading our file
ppr <- pdf_text("assets/wq_burning1.pdf")
###############################################################################
#Text Analysis of a Research Article
###############################################################################
# Setting up libraries
require(librarian)
librarian::shelf(dplyr, tidytext, tidyverse,stringr,stringi,
widyr,igraph, ggraph,
wordcloud, reshape2, graphlayouts,
pluralize, quanteda, qgraph, cowplot, readr, pdftools)
# # Downloading the file
# download.file("https://www.nature.com/articles/s41467-021-22747-3.pdf",
#               destfile = "~/GitHub/burn_iss-website/assets/wq_burning1.pdf", mode = "wb")
# Reading our file
ppr <- pdf_text("assets/wq_burning.pdf")
info <- pdf_info("assets/wq_burning.pdf")
ppr_txt <- data.frame(line = 1:8, text = ppr)
# Extracting pdf metadata and removing it from text body
sbj <- unlist(strsplit(info$keys$Subject, " "))
dmn <- unlist(strsplit(info$keys$`CrossMarkDomains[1]`," "))
ctr <- unlist(strsplit(info$keys$Creator, " "))
doi <- unlist(strsplit(info$keys$doi, " "))
cdm <- unlist(strsplit(info$keys$`CrossMarkDomains[2]`," "))
# Cleaning pdf file
ppr_txt$text <- sub(paste(sbj, collapse = '|'),'',ppr_txt)
ppr_txt$text <- sub(paste(dmn, collapse = '|'),'',ppr_txt)
ppr_txt$text <- sub(paste(ctr, collapse = '|'),'',ppr_txt)
ppr_cln <- as.data.frame(ppr_txt %>%
str_to_lower() %>%
str_replace_all("\n"," ")
ppr_cln <- as.data.frame(ppr_txt %>%
View(ppr_txt)
